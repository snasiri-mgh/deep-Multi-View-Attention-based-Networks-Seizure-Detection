{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"JLMR_shared_codes.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"AJwIwiclzaQE"},"source":["Importing the needed packages:"]},{"cell_type":"code","metadata":{"id":"Fz2cD96n6rhP"},"source":["import scipy.io as sio\n","import numpy as np\n","from tensorflow.keras.models import Model, Sequential\n","from tensorflow.keras.layers import Dense, Activation, Permute, Dropout, Concatenate, Average, Reshape, Multiply\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, AveragePooling1D, Conv1D, MaxPooling1D\n","from tensorflow.keras.layers import SeparableConv2D, DepthwiseConv2D\n","from tensorflow.keras.layers import BatchNormalization\n","from tensorflow.keras.layers import SpatialDropout2D\n","from tensorflow.keras.regularizers import l1_l2\n","from tensorflow.keras.layers import Input, Flatten\n","from tensorflow.keras.constraints import max_norm\n","from tensorflow.keras import backend as K\n","import os\n","import time\n","from sklearn.metrics import confusion_matrix\n","\n","from keras.backend import expand_dims\n","from sklearn.utils.class_weight import compute_class_weight\n","from tensorflow.keras.constraints import NonNeg\n","from sklearn.metrics import classification_report\n","from sklearn import manifold\n","import matplotlib.pyplot as plt\n","from keras.models import model_from_json\n","from tensorflow.keras.utils import plot_model\n","import matplotlib.patches as mpatches\n","from umap.parametric_umap import ParametricUMAP\n","from deepexplain.tensorflow import DeepExplain\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2Gp6ZFiAbQRF"},"source":["The following section implements the hierarchial self-attention mechanism module. This section uses the following implementation:\n","https://github.com/philipperemy/keras-attention-mechanism"]},{"cell_type":"code","metadata":{"id":"zpH6mO8QAPLP"},"source":["# Attention_ViewSelector Class:\n","from tensorflow.keras.layers import Dense, Lambda, dot, Activation, concatenate\n","from tensorflow.keras.layers import Layer\n","\n","\n","class Attention_ViewSelector(Layer):\n","\n","    def __init__(self, **kwargs):\n","        super().__init__(**kwargs)\n","\n","    def __call__(self, hidden_states):\n","        \"\"\"\n","        Many-to-one attention mechanism for Keras.\n","        @param hidden_states: 3D tensor with shape (batch_size, time_steps, input_dim).\n","        @return: 2D tensor with shape (batch_size, 128)\n","        @author: felixhao28.\n","        \"\"\"\n","        hidden_size = int(hidden_states.shape[2])\n","        # Inside dense layer\n","        #              hidden_states            dot               W            =>           score_first_part\n","        # (batch_size, time_steps, hidden_size) dot (hidden_size, hidden_size) => (batch_size, time_steps, hidden_size)\n","        # W is the trainable weight matrix of attention Luong's multiplicative style score\n","        score_first_part = Dense(hidden_size, use_bias=False)(hidden_states)\n","        #            score_first_part           dot        last_hidden_state     => attention_weights\n","        # (batch_size, time_steps, hidden_size) dot   (batch_size, hidden_size)  => (batch_size, time_steps)\n","        h_t = Lambda(lambda x: x[:, -1, :], output_shape=(hidden_size,))(hidden_states)\n","        score = dot([score_first_part, h_t], [2, 1])\n","        attention_weights = Activation('softmax')(score)\n","        # (batch_size, time_steps, hidden_size) dot (batch_size, time_steps) => (batch_size, hidden_size)\n","        context_vector = dot([hidden_states, attention_weights], [1, 1])\n","        return context_vector\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BWUsifUNbeV8"},"source":["In the following section, the vector containing the all subjects' numbers must be loaded:"]},{"cell_type":"code","metadata":{"id":"RrVbPGnQ69Gd"},"source":["#%% <<<<<<<<< num_sub_good >>>>>>>>>>>>>>>>>>>>>>>>>>>\n","num_sub_all = \"vector of the subjects' number\"\n","test_num = len(num_sub_all)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CeT1Eji5b-ms"},"source":["In the next section, three data views and the true labels of all epochs of all subjects must be loaded. Note that we have 22 EEG channels (montages) and our EEG epochs have 3 seconds length, which results in 3 \\times Fs ($3 \\times 250Hz =750$) time samples. The details of our three views are as follows:\n","\n","1) View1: this EEG view is raw denoised EEG time samples having the size of (None, channels=22, EEG samples=750, 1).\n","\n","2) View2: this view is the Time-Frequency (Morse scalogram) representation of the corresponding EEG channel signals of EEG epochs, which are resized to (32*32) frames, resulting in the size of (None, 32, 32, EEG channels=22).\n","\n","3) View3: this view consists of a wide range of the EEG hand-engineered features reported to be efficient in the EEG seizure detection tasks. These features listed in our paper result in 946 features for all channels signals combined (and, therefore, for an EEG epoch) and the total size of (None, 946). \n"]},{"cell_type":"code","metadata":{"id":"1nuQm6YX7BFq"},"source":["#%% <<<<<<<<< MLP NBML features >>>>>>>>>>>>>>>>>>>>>>>>>>>\n","\n","View1 = 'View1 data: Time-sample EEG signals with size of (None, 22, 750, 1)'\n","\n","View2 = 'View2 data: Time-Frequency (Scalogram) images of the EEG signals with size of (None, 32, 32, 22)'\n","\n","View3 = 'View3 data: Hand-engineered features with size of (None, 946)'\n","\n","Labels = 'Labels of epochs of all subjects'\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ap8EIIWAf2v-"},"source":["A folder path for saving the results are specified in the next section."]},{"cell_type":"code","metadata":{"id":"6VydVAvG7EGa"},"source":["path_folder = 'A folder path for save the model weights and results'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1orhUgrXf_YF"},"source":["The following section implements the Leave-One-Subject-Out (LOSO) scheme for obtaining the results on the unseen subject data. The details are provided as follows:\n","\n","*   Section 1) Three data views of the out subject and train subjects are loaded. \n","*   Section 2-4) Three ViewNet (i.e., ViewNet1-3 in our paper) are designed. These networks process the EEG time samples, TF representations, and hand-engineered features, respectively. Note that the ViewNet1 is inspired by [EEGNET](https://github.com/vlawhern/arl-eegmodels).\n","*   Section 5) The ViewNet1-3 are merged using the self-attention module and form the proposed fAttNet in our paper.\n","*   Section 6) Using class weight property, the classification imbalance problem is trying to be addressed.\n","*   Section 7) The proposed fAttNet is trained on 5 epochs on the training subjects using the Categorical cross-entropy as the loss function and Adam as the optimizer. \n","*   Section 8) The weights of the trained fAttNet can be saved in this section. The absolute weights corresponding to the spatial filter, having 22 elements, of the ViewNet1 part of the fAttNet can be averaged over all trained fAttNets on all unseen subjects to show the topography of the **Figure 15** in our paper. Also, the spectrograms of the weights corresponding to the temporal filters of an example trained fAttNet can result in the **Figure 14** in our paper.\n","*   Section 9) The confusion matrix and different classification metrics (i.e., Accuracy, Sensitivity, Specificity, Precision, and F1-Score) corresponding to the prediction of the trained fAttNet on the unseen subject are obtained can be accumulated with the obtained previous results on the previous unseen subjects to yield the final results on all unseen subjects, which are reported in **Tables 1 and 2** in our paper.\n","*   Section 10) In this section, the outputs of the final flatten layer of the fAttNet on the unseen subject data are obtained to be ready for the t-SNE and parametric UMAP feature reduction methods.\n","*   Section 11) Using t-SNE, the obtained embedding in Section 10 is reduced to the 2 dimensions and is visualized to show the discriminability aspect of the proposed fAttNet. These results are shown in **Figure 17** (top row) in our paper.\n","*   Section 12) Similar to the previous section (i.e., Section 11) but using the parametric UMAP method. Note that for implementing the non-linear embedding in this method, we use a fully connected network with 100 neurons. These results are shown in **Figure 17** (top row) in our paper.\n","*   Section 13) Finally, to obtain the saliency maps on the unseen data subjects, we have used the [Deep Explain](https://github.com/marcoancona/DeepExplain) library. Using this section, one can obtain the saliency maps corresponding to all EEG epochs. Note that for obtaining the results shown in the **Figure 16** of our paper, we just executed the obtained saliency maps corresponding to the View-Net1 part of the fAttNet."]},{"cell_type":"code","metadata":{"id":"WCl-mmzJ7Kf2"},"source":["for Out_Subject in np.arange(0, len(num_sub_all)):\n","    \n","    print('\\nSubject_out: ' + str(Out_Subject) + '(' + str(num_sub_all[Out_Subject]) + ')' + '/'+ str(len(num_sub_all))+' started...')\n","        \n","    \n","#%% Section 1: #########################################3###############################################################################################################################:\n","    View1_test = 'View1 test data: data of the out subject'\n","    View3_test = 'View3 test data: data of the out subject'\n","    View2_test = 'View2 test data: data of the out subject'\n","    labels_test = \"labels of the out subject's epochs\"\n","\n","    View1_train = 'View1 train data: data of the train subjects'\n","    View3_train = 'View2 train data: data of the train subjects'\n","    View2_train = 'View3 train data: data of the train subjects'\n","    labels_train = \"labels of the train subjects' epochs\"   \n","    \n","    \n","#%% Section 2: View-Net1: EEGNET model #########################################3###############################################################################################################################:\n","    nb_classes = 2\n","    Chans = 22\n","    Samples = 750\n","    dropoutRate = 0.5\n","    kernLength = 32\n","    F1 = 4\n","    D = 1\n","    F2 = F1*D \n","    dropoutType = 'Dropout'\n","    norm_rate = 0.25\n","    Concat_Dense = 16\n","        \n","    input1   = Input(shape = (Chans, Samples, 1))\n","\n","    block1       = Conv2D(F1, (1, kernLength), padding = 'same',\n","                                   input_shape = (Chans, Samples, 1),\n","                                   use_bias = False)(input1)\n","    block1       = BatchNormalization(axis = 1)(block1)\n","    block1       = DepthwiseConv2D((Chans, 1), use_bias = False, \n","                                   depth_multiplier = D,\n","                                   depthwise_constraint = max_norm(1.))(block1)\n","    block1       = BatchNormalization(axis = 1)(block1)\n","    block1       = Activation('elu')(block1)\n","    block1       = AveragePooling2D((1, 4))(block1)\n","    block1       = Dropout(dropoutRate)(block1)\n","        \n","    block2       = SeparableConv2D(F2, (1, 16), use_bias = False, padding = 'same')(block1)\n","    block2       = BatchNormalization(axis = 1)(block2)\n","    block2       = Activation('elu')(block2)\n","    block2       = AveragePooling2D((1, 4))(block2)\n","    block2       = Dropout(dropoutRate)(block2)\n","        \n","    flatten1      = Flatten()(block2)\n","    flatten1        = Dense(Concat_Dense)(flatten1)\n","    flatten1_temp = Reshape((1, flatten1.shape[-1]))(flatten1)\n","    \n","    dense_model1        = Dense(2, kernel_constraint = max_norm(norm_rate))(flatten1)\n","    softmax1      = Activation('softmax')(dense_model1)\n","\n","    model1 = Model(inputs=input1, outputs=softmax1)\n","    model1_Embedder = Model(inputs=input1, outputs=flatten1)\n","\n","    softmax1_temp = Reshape((softmax1.shape[-1],1))(softmax1)\n","    \n","    \n","#%% Section 3: View-Net2: CNN-image model ######################################################################################################################################################################3:\n","        \n","    input2   = Input(shape = (View2_test.shape[1], View2_test.shape[2], View2_test.shape[3]))\n","\n","    block1       = BatchNormalization(axis = 1)(input2)\n","    block1       = Conv2D(32, (7, 7), padding = 'valid',\n","                                   input_shape = (View2_test.shape[1], View2_test.shape[2], View2_test.shape[3]))(block1)    \n","    block1       = BatchNormalization(axis = 1)(block1)\n","    block1       = Activation('elu')(block1)\n","    block1       = Conv2D(32, (7, 7), padding = 'valid')(block1)\n","    block1       = BatchNormalization(axis = 1)(block1)\n","    block1       = Activation('elu')(block1)\n","    block1       = MaxPooling2D((2, 2))(block1)\n","    block1       = Dropout(dropoutRate)(block1)\n","    \n","    block2       = Conv2D(32, (5, 5), padding = 'valid')(block1)\n","    block2       = BatchNormalization(axis = 1)(block2)\n","    block2       = Activation('elu')(block2)\n","    block2       = Conv2D(Concat_Dense, (5, 5), padding = 'valid')(block2)\n","    block2       = BatchNormalization(axis = 1)(block2)\n","    block2       = Activation('elu')(block2)\n","    block2       = MaxPooling2D((2, 2))(block2)\n","    block2       = Dropout(dropoutRate)(block2)\n","        \n","\n","    flatten2      = Flatten()(block2)\n","    flatten2_temp = Reshape((1, flatten2.shape[-1]))(flatten2)\n","    \n","    dense_model2        = Dense(2, kernel_constraint = max_norm(norm_rate))(flatten2)\n","    softmax2      = Activation('softmax')(dense_model2)\n","\n","    model2 = Model(inputs=input2, outputs=softmax2)\n","    model2_Embedder = Model(inputs=input2, outputs=flatten2)\n","    \n","    softmax2_temp = Reshape((softmax2.shape[-1],1))(softmax2)\n","    \n","#%% Section 4: View-Net3: for processing Hand_engineered features #####################################################################################3###################################################################################3:\n","    Hand_engineered_features_num = 946\n","        \n","    input3   = Input(shape = (Hand_engineered_features_num,))\n","\n","    block1       = BatchNormalization(axis = 1)(input3)\n","    block1       = Dense(Concat_Dense, input_shape = (Chans,))(block1)\n","    block1       = BatchNormalization(axis = 1)(block1)\n","    block1       = Activation('elu')(block1)\n","    block1       = Dropout(dropoutRate)(block1)\n","    \n","\n","    flatten3      = Flatten()(block1)\n","    flatten3_temp = Reshape((1, flatten3.shape[-1]))(flatten3)\n","    \n","    dense_model3        = Dense(2, kernel_constraint = max_norm(norm_rate))(flatten3)\n","    softmax3      = Activation('softmax')(dense_model3)\n","\n","    model3 = Model(inputs=input3, outputs=softmax3)\n","    model3_Embedder = Model(inputs=input3, outputs=flatten3)\n","\n","    softmax3_temp = Reshape((softmax3.shape[-1],1))(softmax3)\n","    \n","#%% Section 5: Merged Models ########################################################################################################################################3 \n","\n","    dense = Concatenate(axis=1)([flatten1_temp, flatten2_temp, flatten3_temp])  \n","    flatten4_1 = Attention_ViewSelector()(dense)\n","    \n","    dense = Concatenate(axis=1)([flatten1_temp, flatten3_temp, flatten2_temp])  \n","    flatten4_2 = Attention_ViewSelector()(dense)\n","\n","    dense = Concatenate(axis=1)([flatten2_temp, flatten1_temp, flatten3_temp])  \n","    flatten4_3 = Attention_ViewSelector()(dense)\n","\n","    dense = Concatenate(axis=1)([flatten2_temp, flatten3_temp, flatten1_temp])  \n","    flatten4_4 = Attention_ViewSelector()(dense)\n","\n","    dense = Concatenate(axis=1)([flatten3_temp, flatten1_temp, flatten2_temp])  \n","    flatten4_5 = Attention_ViewSelector()(dense)\n","\n","    dense = Concatenate(axis=1)([flatten3_temp, flatten2_temp, flatten1_temp])  \n","    flatten4_6 = Attention_ViewSelector()(dense)\n","    \n","    dense = Concatenate()([flatten4_1, flatten4_2, flatten4_3, flatten4_4, flatten4_5, flatten4_6])\n","    dense       = Dropout(dropoutRate)(dense)\n","    flatten4      = Flatten()(dense)\n","\n","    dense        = Dense(2, kernel_constraint = max_norm(norm_rate))(flatten4)\n","    softmax      = Activation('softmax')(dense)\n","    model = Model(inputs=[input1, input2, input3], outputs=[softmax])\n","\n","    plot_model(model, to_file=str(path_folder + 'model.png'), show_shapes=True)\n","\n","    model_Embedder = Model(inputs=[input1, input2, input3], outputs=flatten4)\n","    \n","#%% Section 6: Using class weight to balance the classification problem : #######################################################################################################################33\n","    \n","    Class_weights = compute_class_weight('balanced', classes = np.unique(np.argmax(labels_train,1)),\n","                                         y = np.argmax(labels_train,1))\n","    class_weights = dict(enumerate(Class_weights)) \n","       \n","#%% Section 7: compile and train the fAttNet model: ##################################################################################################################################\n","    model.compile(loss=['categorical_crossentropy'], optimizer='adam', \n","              metrics = ['accuracy'])\n","    history = model.fit([View1_train, View2_train, View3_train], [labels_train], epochs=5, batch_size=32,shuffle=True,class_weight=class_weights)#,verbose=1,validation_split=0.15,callbacks=callbacks_list)\n","    \n","#%% Section 8: Save the trained fAttNet's weights:\n","\n","#    model.save_weights(path_folder+'model_weights_sub'+str(Out_Subject)+'.h5')\n","    model.load_weights(path_folder+'model_weights_sub'+str(Out_Subject)+'.h5')\n","        \n","#%% Section 9: show the classification results and metrics (Confusion matrice and Acc, Sen, Spec, Prec and F1-Score):\n","    \n","    View3_test = np.squeeze(View3_test)\n","    \n","    labels_test_predict = model.predict([View1_test, View2_test, View3_test],batch_size=16)\n","            \n","    y_true = np.argmax(labels_test, 1)\n","    \n","    y_pred =  np.argmax(labels_test_predict, 1)\n","        \n","    C = confusion_matrix(y_true, y_pred, labels=[0,1]) # similar to this, produce confusion matrices of the ViewNet1-3\n","    \n","    print('\\nSubject_out: ' + str(Out_Subject) + '/'+str(test_num)+' >>>>>>>>>>>> fAttNet model results: \\n'+ str(C))\n","       \n","    print(classification_report(y_true, y_pred))\n","\n","    \n","#%% Section 10: obtain embedded test data for visualizing the t-SNE and UMAP scatter plots:\n","    model1_Embedder_pred_reduced = model1_Embedder.predict(View1_test, batch_size=16)          \n","    model2_Embedder_pred_reduced = model2_Embedder.predict(View2_test, batch_size=16)            \n","    model3_Embedder_pred_reduced = model3_Embedder.predict(View3_test, batch_size=16)            \n","    model_Embedder_pred_reduced = model_Embedder.predict([View1_test, View2_test, View3_test], batch_size=16)\n","\n","#%% Section 11: tSNE:\n","#    # get 2d embeddings\n","    tsne = manifold.TSNE(n_components=2)\n","    model1_Embedder_pred_reduced_tSNE = tsne.fit_transform(model1_Embedder_pred_reduced)\n","    \n","    tsne = manifold.TSNE(n_components=2)\n","    model2_Embedder_pred_reduced_tSNE = tsne.fit_transform(model2_Embedder_pred_reduced)\n","\n","    tsne = manifold.TSNE(n_components=2)\n","    model3_Embedder_pred_reduced_tSNE = tsne.fit_transform(model3_Embedder_pred_reduced)\n","    \n","    tsne = manifold.TSNE(n_components=2)\n","    model_Embedder_pred_reduced_tSNE = tsne.fit_transform(model_Embedder_pred_reduced)\n","\n","    # t-SNE subplot:\n","    color_scatter = []\n","    for i in range(len(y_true)):\n","        output_class = y_true[i]\n","        if(output_class == 0):\n","            color_scatter.append(\"#0000ff\") # Blue ====> Class: 0\n","        else:\n","            color_scatter.append(\"#ff0000\") # Red ====> Class: 1\n","                                       \n","    fig, axs = plt.subplots(2, 4)\n","        \n","    C = \"confusion matrix of the View-Net1's prediction\"\n","    acc = np.round(100*(C[0,0]+C[1,1])/(C[0,0]+C[1,1]+C[1,0]+C[0,1]))\n","    axs[1, 0].scatter(x = model1_Embedder_pred_reduced_tSNE[:, 0], \n","                y=model1_Embedder_pred_reduced_tSNE[:, 1],\n","                color=color_scatter)\n","    axs[1, 0].set_title(str('View-Net1'))\n","    pop_a = mpatches.Patch(color='#0000ff', label='Non-seizure')\n","    pop_b = mpatches.Patch(color='#ff0000', label='Seizure')\n","    axs[1, 0].legend(handles=[pop_a,pop_b])\n","    axs[1, 0].set(xlabel='t-SNE 1', ylabel='t-SNE 2')\n","    np.save(str(path_folder + 'Sub'+str(Out_Subject)+'_model1_Embedder_pred_reduced_tSNE.npy'), model1_Embedder_pred_reduced_tSNE)\n","      \n","    C = \"confusion matrix of the View-Net2's prediction\"\n","    acc = np.round(100*(C[0,0]+C[1,1])/(C[0,0]+C[1,1]+C[1,0]+C[0,1]))\n","    axs[1, 1].scatter(x = model2_Embedder_pred_reduced_tSNE[:, 0], \n","                y=model2_Embedder_pred_reduced_tSNE[:, 1],\n","                color=color_scatter)\n","    axs[1, 1].set_title(str('View-Net2'))\n","    axs[1, 1].legend(handles=[pop_a,pop_b])\n","    axs[1, 1].set(xlabel='t-SNE 1', ylabel='t-SNE 2')\n","    np.save(str(path_folder + 'Sub'+str(Out_Subject)+'_model2_Embedder_pred_reduced_tSNE.npy'), model2_Embedder_pred_reduced_tSNE)\n","\n","    C = \"confusion matrix of the View-Net3's prediction\"\n","    acc = np.round(100*(C[0,0]+C[1,1])/(C[0,0]+C[1,1]+C[1,0]+C[0,1]))\n","    axs[1, 2].scatter(x = model3_Embedder_pred_reduced_tSNE[:, 0], \n","                y=model3_Embedder_pred_reduced_tSNE[:, 1],\n","                color=color_scatter)\n","    axs[1, 2].set_title(str('View-Net3'))\n","    axs[1, 2].legend(handles=[pop_a,pop_b])\n","    axs[1, 2].set(xlabel='t-SNE 1', ylabel='t-SNE 2')\n","    np.save(str(path_folder + 'Sub'+str(Out_Subject)+'_model3_Embedder_pred_reduced_tSNE.npy'), model3_Embedder_pred_reduced_tSNE)\n","\n","    C = \"confusion matrix of the fAttNet's prediction\"\n","    acc = np.round(100*(C[0,0]+C[1,1])/(C[0,0]+C[1,1]+C[1,0]+C[0,1]))\n","    axs[1, 3].scatter(x = model_Embedder_pred_reduced_tSNE[:, 0], \n","                y=model_Embedder_pred_reduced_tSNE[:, 1],\n","                color=color_scatter)\n","    axs[1, 3].set_title(str('fAttNet'))\n","    axs[1, 3].legend(handles=[pop_a,pop_b])\n","    axs[1, 3].set(xlabel='t-SNE 1', ylabel='t-SNE 2')\n","    np.save(str(path_folder + 'Sub'+str(Out_Subject)+'_model_Embedder_pred_reduced_tSNE.npy'), model_Embedder_pred_reduced_tSNE)\n","\n","#%% Section 12: UMAP: \n","    dims = (int(flatten1.shape[1]),)\n","    n_components = 2\n","    encoder = Sequential([\n","       Input(input_shape=dims),\n","       Dense(units=n_components),\n","    ])\n","   \n","    encoder1 = encoder\n","    encoder2 = encoder\n","    encoder3 = encoder    \n","   \n","   \n","    embedder = ParametricUMAP(encoder=encoder1, dims=dims)    \n","    model1_Embedder_pred_reduced_UMAP = embedder.fit_transform(model1_Embedder_pred_reduced)\n","    \n","    dims = (int(flatten2.shape[1]),)\n","    n_components = 2\n","    encoder = Sequential([\n","       Input(input_shape=dims),\n","       Dense(units=n_components),\n","    ])\n","   \n","    encoder2 = encoder\n","    \n","    embedder = ParametricUMAP(encoder=encoder2, dims=dims)    \n","    model2_Embedder_pred_reduced_UMAP = embedder.fit_transform(model2_Embedder_pred_reduced)\n","\n","    dims = (int(flatten3.shape[1]),)\n","    n_components = 2\n","    encoder = Sequential([\n","       Input(input_shape=dims),\n","       Dense(units=n_components),\n","    ])\n","   \n","    encoder3 = encoder\n","    \n","    embedder = ParametricUMAP(encoder=encoder3, dims=dims)    \n","    model3_Embedder_pred_reduced_UMAP = embedder.fit_transform(model3_Embedder_pred_reduced)\n","\n","    dims = (int(flatten4.shape[1]),)\n","    n_components = 2\n","    encoder = Sequential([\n","       Input(input_shape=dims),\n","       Dense(units=n_components),\n","    ])\n","   \n","    encoder4 = encoder\n","    \n","    embedder = ParametricUMAP(encoder=encoder4, dims=dims)    \n","    model_Embedder_pred_reduced_UMAP = embedder.fit_transform(model_Embedder_pred_reduced)\n","\n","## UMAP subplot:\n","    color_scatter = []\n","    for i in range(len(y_true)):\n","        output_class = y_true[i]\n","        if(output_class == 0):\n","            color_scatter.append(\"#0000ff\") # Blue ====> Class: 0\n","        else:\n","            color_scatter.append(\"#ff0000\") # Red ====> Class: 1\n","                                        \n","\n","    C = \"confusion matrix of the View-Net1's prediction\"\n","    acc = np.round(100*(C[0,0]+C[1,1])/(C[0,0]+C[1,1]+C[1,0]+C[0,1]))\n","    axs[0, 0].scatter(x = model1_Embedder_pred_reduced_UMAP[:, 0], \n","                y=model1_Embedder_pred_reduced_UMAP[:, 1],\n","                color=color_scatter)\n","    axs[0, 0].set_title(str('View-Net1'))\n","    axs[0, 0].legend(handles=[pop_a,pop_b])\n","    axs[0, 0].set(xlabel='UMAP 1', ylabel='UMAP 2')\n","    np.save(str(path_folder + 'Sub'+str(Out_Subject)+'_model1_Embedder_pred_reduced_UMAP.npy'), model1_Embedder_pred_reduced_UMAP)\n","\n","    C = \"confusion matrix of the View-Net2's prediction\"\n","    acc = np.round(100*(C[0,0]+C[1,1])/(C[0,0]+C[1,1]+C[1,0]+C[0,1]))\n","    axs[0, 1].scatter(x = model2_Embedder_pred_reduced_UMAP[:, 0], \n","                y=model2_Embedder_pred_reduced_UMAP[:, 1],\n","                color=color_scatter)\n","    axs[0, 1].set_title(str('View-Net2'))\n","    axs[0, 1].legend(handles=[pop_a,pop_b])\n","    axs[0, 1].set(xlabel='UMAP 1', ylabel='UMAP 2')\n","    np.save(str(path_folder + 'Sub'+str(Out_Subject)+'_model2_Embedder_pred_reduced_UMAP.npy'), model2_Embedder_pred_reduced_UMAP)\n","\n","    C = \"confusion matrix of the View-Net3's prediction\"\n","    acc = np.round(100*(C[0,0]+C[1,1])/(C[0,0]+C[1,1]+C[1,0]+C[0,1]))\n","    axs[0, 2].scatter(x = model3_Embedder_pred_reduced_UMAP[:, 0], \n","                y=model3_Embedder_pred_reduced_UMAP[:, 1],\n","                color=color_scatter)\n","    axs[0, 2].set_title(str('View-Net3'))\n","    axs[0, 2].legend(handles=[pop_a,pop_b])\n","    axs[0, 2].set(xlabel='UMAP 1', ylabel='UMAP 2')\n","    np.save(str(path_folder + 'Sub'+str(Out_Subject)+'_model3_Embedder_pred_reduced_UMAP.npy'), model3_Embedder_pred_reduced_UMAP)\n","\n","    C = \"confusion matrix of the fAttNet's prediction\"\n","    acc = np.round(100*(C[0,0]+C[1,1])/(C[0,0]+C[1,1]+C[1,0]+C[0,1]))\n","    axs[0, 3].scatter(x = model_Embedder_pred_reduced_UMAP[:, 0], \n","                y=model_Embedder_pred_reduced_UMAP[:, 1],\n","                color=color_scatter)\n","    axs[0, 3].set_title(str('fAttNet'))\n","    axs[0, 3].legend(handles=[pop_a,pop_b])\n","    axs[0, 3].set(xlabel='UMAP 1', ylabel='UMAP 2')\n","    np.save(str(path_folder + 'Sub'+str(Out_Subject)+'_model_Embedder_pred_reduced_UMAP.npy'), model_Embedder_pred_reduced_UMAP)\n","\n","    # Hide x labels and tick labels for top plots and y ticks for right plots.\n","    for ax in axs.flat:\n","        ax.label_outer()\n","\n","    fig.savefig(path_folder + 'Sub'+str(Out_Subject)+'.png')\n","    \n","#%% Section 13: Obtain saliency maps: ======>>>> requirements: install DeepExplain library (in the appendix files), Keras 2.2.4, Tensorflow 1.10.0 \n","    \n","    with DeepExplain(session=K.get_session()) as de:  # <-- init DeepExplain context\n","        # 1. Get the input tensor to the original model\n","        input_tensor = model1.layers[0].input\n","        \n","        # 2. We now target the output of the last dense layer (pre-softmax)\n","        # To do so, create a new model sharing the same layers untill the last dense (index -2)\n","        fModel = Model(inputs=input_tensor, outputs = model1.layers[-2].output)\n","        target_tensor = fModel(input_tensor)\n","        \n","        xs = View1_test\n","        ys = labels_test\n","        \n","        attributions_sal   = de.explain('saliency', target_tensor, input_tensor, xs=xs, ys=ys, batch_size=32)\n","        \n","        print (\"Done\") \n","        \n","    np.save(path_folder+'Sub'+str(Out_Subject)+'_attributions_sal.npy', attributions_sal)        \n"],"execution_count":null,"outputs":[]}]}